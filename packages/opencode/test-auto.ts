#!/usr/bin/env bun

// Test script for the auto interface
import { AutoTaskTool } from "./src/auto/auto-interface"
import { Config } from "./src/config/config"

async function testAutoInterface() {
  console.log("üß™ Testing OpenCode Auto Interface...")

  try {
    // Get the config to ensure auto-selection is enabled
    const config = await Config.get()

    if (!config.experimental?.autoSelection?.enabled) {
      console.error("‚ùå Auto-selection is not enabled in config")
      process.exit(1)
    }

    console.log("‚úÖ Auto-selection is enabled")
    console.log(`üìä Confidence threshold: ${config.experimental.autoSelection.confidence}`)
    console.log(`üîÑ Parallel threshold: ${config.experimental.autoSelection.parallelThreshold}`)
    console.log(`üöÄ Max parallel tasks: ${config.experimental.autoSelection.maxParallelTasks}`)

    // Create mock context
    const mockCtx = {
      sessionID: "test-session",
      userID: "test-user",
    }

    // Test different task types
    const testTasks = [
      {
        prompt: "Add a simple console.log statement",
        expectedType: "coding",
        expectedComplexity: "simple",
      },
      {
        prompt: "Explore the codebase structure and identify authentication patterns",
        expectedType: "exploration",
        expectedComplexity: "medium",
      },
      {
        prompt:
          "Design and implement a complete microservices architecture with API gateway, authentication service, and database migration system",
        expectedType: "planning",
        expectedComplexity: "complex",
      },
    ]

    console.log("\nüéØ Testing task analysis...")

    for (const task of testTasks) {
      console.log(`\nüìù Task: "${task.prompt}"`)

      try {
        // This would normally call the auto tool, but we'll test the selection logic directly
        const { AutoSelector } = await import("./src/auto/auto-selector")
        const selection = await AutoSelector.getAutoSelection(task.prompt)

        console.log(`‚úÖ Analyzed successfully:`)
        console.log(`   üéØ Type: ${selection.analysis.taskType}`)
        console.log(`   üìä Complexity: ${selection.analysis.complexity}`)
        console.log(`   ü§ñ Agent: ${selection.agent}`)
        console.log(`   üß† Model: ${selection.model.providerID}/${selection.model.modelID}`)
        console.log(`   üìà Confidence: ${(selection.analysis.confidence * 100).toFixed(1)}%`)

        // Check if it matches expectations
        if (selection.analysis.taskType === task.expectedType) {
          console.log(`   ‚úÖ Task type matches expected: ${task.expectedType}`)
        } else {
          console.log(`   ‚ö†Ô∏è  Task type mismatch: expected ${task.expectedType}, got ${selection.analysis.taskType}`)
        }

        if (selection.analysis.complexity === task.expectedComplexity) {
          console.log(`   ‚úÖ Complexity matches expected: ${task.expectedComplexity}`)
        } else {
          console.log(
            `   ‚ö†Ô∏è  Complexity mismatch: expected ${task.expectedComplexity}, got ${selection.analysis.complexity}`,
          )
        }
      } catch (error) {
        console.error(`   ‚ùå Analysis failed: ${error.message}`)
      }
    }

    console.log("\nüéâ Auto interface test completed!")
  } catch (error) {
    console.error("‚ùå Test failed:", error.message)
    console.error(error.stack)
    process.exit(1)
  }
}

// Run the test
testAutoInterface().catch(console.error)
